# -*- coding: utf-8 -*-
"""Zadanie1_simple_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sOZ3AwxBoWPrQ_OnxH8ZOPQlxAmocX5E
"""

from dotenv import load_dotenv
import os
from langchain_ollama import ChatOllama
from langchain_ollama import OllamaEmbeddings
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from langsmith.wrappers import wrap_openai
from langsmith import traceable

load_dotenv()

def get_llm():
    llm_type = os.getenv("LLM_TYPE", "groq")
    if llm_type == "ollama":
        return ChatOllama(model="llama3.1:8b-instruct-q4_0", temperature=0)
    elif llm_type == "groq":
        print("Using GROQ")
        return ChatGroq(temperature=0.1, model_name="llama-3.1-70b-versatile")
    else:
        return ChatOpenAI(temperature=0, model="gpt-4o-mini")

def get_embeddings():
    embedding_type = os.getenv("LLM_TYPE", "groq")
    if embedding_type == "ollama":
        return OllamaEmbeddings(model="llama3.1:8b-instruct-q4_0")
    else:
        return OpenAIEmbeddings()

"""#### Creating VectorDatabase"""

from langchain.schema import Document
from langchain_community.vectorstores import Chroma

embedding_function = get_embeddings()

docs = [
    Document(
        page_content="Bella Vista is owned by Antonio Rossi, a renowned chef with over 20 years of experience in the culinary industry. He started Bella Vista to bring authentic Italian flavors to the community.",
        metadata={"source": "restaurant_info.txt"},
    ),
    Document(
        page_content="Bella Vista offers a range of dishes with prices that cater to various budgets. Appetizers start at $8, main courses range from $15 to $35, and desserts are priced between $6 and $12.",
        metadata={"source": "restaurant_info.txt"},
    ),
    Document(
        page_content="Bella Vista is open from Monday to Sunday. Weekday hours are 11:00 AM to 10:00 PM, while weekend hours are extended from 11:00 AM to 11:00 PM.",
        metadata={"source": "restaurant_info.txt"},
    ),
    Document(
        page_content="Bella Vista offers a variety of menus including a lunch menu, dinner menu, and a special weekend brunch menu. The lunch menu features light Italian fare, the dinner menu offers a more extensive selection of traditional and contemporary dishes, and the brunch menu includes both classic breakfast items and Italian specialties.",
        metadata={"source": "restaurant_info.txt"},
    ),
]

path_to_file="C:\\Users\\wotmi\\OneDrive\\Pulpit\\Github\\simple_rag\\" ##input("Enter the path to the directory: ")
file_name="A Human-in-the-Loop Robot Grasping System with Grasp Quality Refi.pdf" ##input("Enter the name of the file: ")

def load_pdf(file_name, file_path):
    loader = PyPDFLoader(file_path+file_name)
    pdf_pages = loader.load()
    return pdf_pages

def split_text(text):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    all_splits_pypdf = splitter.split_documents(text)
    all_splits_pypdf_texts = [d.page_content for d in all_splits_pypdf]
    return all_splits_pypdf_texts

db = Chroma.from_texts(# add metadata to the documents
    texts=split_text(load_pdf(file_name=file_name, file_path=path_to_file)),
    collection_name="baseline",
    embedding=embedding_function
)
retriever = db.as_retriever(search_kwargs={'k': 8})
#db = Chroma.from_documents(docs, embedding_function)
#retriever = db.as_retriever()

from typing_extensions import TypedDict


class AgentState(TypedDict):
    prompt: str
    #grades: list[str]
    llm_output: str
    documents: list[str]
    classifications: list[str]
    #on_topic: bool

def retrieve_docs(state: AgentState):
    prompt = state["prompt"]
    print("Prompt: "+prompt)
    documents = retriever.invoke(input=prompt)
    print("RETRIEVED DOCUMENTS:", documents)
    state["documents"] = [doc.page_content for doc in documents]
    return state

from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate


class Gradeprompt(BaseModel):
    """Boolean value to check whether a prompt is releated to the thesis paper."""

    score: str = Field(
        description="prompt is about text classification of the thesis paper? If yes -> 'Yes' if not -> 'No'"
    )


def prompt_classifier(state: AgentState):
    prompt = state["prompt"]

    # system = """You are a grader assessing the topic a user prompt. \n
    #     <instructions>
    #     Only answer if the prompt is about one of the following topics:
    #     1. Information about the owner of Bella Vista (Antonio Rossi).
    #     2. Prices of dishes at Bella Vista.
    #     3. Opening hours of Bella Vista.
    #     4. Available menus at Bella Vista.
    #     </instructions>

    #     <Examples>
    #      How will the weather be today -> No
    #               Who owns the restaurant? -> Yes
    #               What food do you offer? -> Yes
    #     </Examples>
    #     If the prompt IS about these topics response with "Yes", otherwise respond with "No".
    #     """
    system="""You are a grader assesing the type of information in a paper. \n
    <instructions>
    Asses if the information you just read is a technical information or a scientific one.
    </instructions>
    
    <Examples>
    Technical Information -> Methods used, procedures, specifications of some machines, devices, etc.
    Scientific Information -> Theoretical concepts, scenarios and mathematical models, graphs and experiments with conclusions etc.
    </Examples>
    
    If the data falls within the technical information category, respond with "The provided piece of information is technical".
    If the data falls within the scientific information category, respond with "The provided piece of information is scientific".
    """

    # grade_prompt = ChatPromptTemplate.from_messages(
    #     [
    #         ("system", system),
    #         ("human", "User: Classify the data from the given paper as technical or scientific."),
    #     ]
    # )
    classification_prompt = ChatPromptTemplate.from_messages([
        ("system", system),
        ("human", "{document}")
    ])

    llm = get_llm()
    classifications = []
    for doc in state["documents"]:
        chain = classification_prompt | llm | StrOutputParser()
        result = chain.invoke({"document": doc})
        classifications.append(result)

    state["classifications"] = classifications
    #structured_llm = llm.with_structured_output(Gradeprompt)
    #grader_llm = grade_prompt #| structured_llm
    #result = grader_llm.invoke({"prompt": prompt})
    #print(f"prompt and GRADE: {prompt} - {result.score}")
    #state["on_topic"] = result.score
    return state

def on_topic_router(state: AgentState):
    on_topic = state["on_topic"]
    if on_topic.lower() == "yes":
        return "on_topic"
    return "off_topic"

def off_topic_response(state: AgentState):
    state["llm_output"] = "I cant respond to that!"
    return state

class GradeDocuments(BaseModel):
    """Boolean values to check for relevance on retrieved documents."""

    score: str = Field(
        description="Documents are relevant to the prompt, 'Yes' or 'No'"
    )


def document_grader(state: AgentState):
    docs = state["documents"]
    prompt = state["prompt"]

    system = """You are a grader assessing relevance of a retrieved document to a user prompt. \n
        If the document contains semantic meaning related to the prompt, grade it as relevant. \n
        Give a binary score 'Yes' or 'No' score to indicate whether the document is relevant to the prompt."""

    grade_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            (
                "human",
                "Retrieved document: \n\n {document} \n\n User prompt: {prompt}",
            ),
        ]
    )

    llm = get_llm()
    structured_llm = llm.with_structured_output(GradeDocuments)
    grader_llm = grade_prompt | structured_llm
    scores = []
    for doc in docs:
        result = grader_llm.invoke({"document": doc, "prompt": prompt})
        scores.append(result.score)
    state["grades"] = scores
    return state

def gen_router(state: AgentState):
    grades = state["grades"]
    print("DOCUMENT GRADES:", grades)

    if any(grade.lower() == "yes" for grade in grades):
        filtered_grades = [grade for grade in grades if grade.lower() == "yes"]
        print("FILTERED DOCUMENT GRADES:", filtered_grades)
        return "generate"
    else:
        return "rewrite_query"

from langchain_core.output_parsers import StrOutputParser


def rewriter(state: AgentState):
    prompt = state["prompt"]
    system = """You are a prompt re-writer that converts an input prompt to a better version that is optimized \n
        for retrieval. Look at the input and try to reason about the underlying semantic intent / meaning."""
    re_write_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            (
                "human",
                "Here is the initial prompt: \n\n {prompt} \n Formulate an improved prompt.",
            )
        ]
    )
    llm = get_llm()
    prompt_rewriter = re_write_prompt | llm | StrOutputParser()
    output = prompt_rewriter.invoke({"prompt": prompt})
    state["prompt"] = output
    return state

from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

@traceable
def generate_answer(state: AgentState):
    llm = get_llm()
    prompt = state["prompt"]
    #context = state["documents"]
    context = "\n\n".join([f"{doc} -> Classified as {cls}" for doc, cls in zip(state["documents"], state["classifications"])])


    #template = """Answer the prompt based only on the following context:
    #{context}

    #prompt: {prompt}
    #"""
    template = """
    Based on the following context, provide a classification of the information in the given paper.:
    {context}
    """

    prompt = ChatPromptTemplate.from_template(
        template=template,
    )
    chain = prompt | llm | StrOutputParser()
    result = chain.invoke({"prompt": prompt, "context": context})
    state["llm_output"] = result
    return state

from langgraph.graph import StateGraph, END

workflow = StateGraph(AgentState)

#workflow.add_node("topic_decision", prompt_classifier)
#workflow.add_node("off_topic_response", off_topic_response)
workflow.add_node("retrieve_docs", retrieve_docs)
workflow.add_node("topic_classification", prompt_classifier)
#workflow.add_node("rewrite_query", rewriter)
workflow.add_node("generate_answer", generate_answer)
#workflow.add_node("document_grader", document_grader)

#workflow.add_edge("off_topic_response", END)
workflow.add_edge("retrieve_docs", "topic_classification")
workflow.add_edge("topic_classification", "generate_answer")
# workflow.add_conditional_edges(
#     "topic_decision",
#     on_topic_router,
#     {
#         "on_topic": "retrieve_docs",
#         "off_topic": "off_topic_response",
#     },
# )
# workflow.add_conditional_edges(
#     "document_grader",
#     gen_router,
#     {
#         "generate": "generate_answer",
#         "rewrite_query": "rewrite_query",
#     },
# )
#workflow.add_edge("rewrite_query", "retrieve_docs")
workflow.add_edge("generate_answer", END)


workflow.set_entry_point("retrieve_docs")

app = workflow.compile()

from IPython.display import Image, display

try:
    display(Image(app.get_graph(xray=True).draw_mermaid_png()))
except:
    pass

result = app.invoke({"prompt": "Machine learning"})## add chat rewriter to change the prompt
print(result["llm_output"])

# result = app.invoke({"prompt": "Who is the owner of bella vista?"})
# print(result["llm_output"])